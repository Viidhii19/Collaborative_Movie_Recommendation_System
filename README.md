# Collaborative_Movie_Recommendation_System
## ğŸ“Œ Project Overview

This project implements a **collaborative movie recommendation system** using the **Netflix Prize Dataset** and a **Transformer architecture**.

Instead of traditional recommendation methods (like KNN or SVD), this system models **user watch history as a sequence**, similar to how Large Language Models (LLMs) model text.

> **Key idea:**
> Movies = tokens
> User history = sentence
> Predict next movie = next-token prediction (GPT logic)

This project is designed to **deeply understand Transformers and LLM fundamentals**, not just to build a recommender.

---

## ğŸ¯ Objectives

* Understand collaborative filtering from first principles
* Learn how **sequential recommendation** works
* Apply **Transformer architecture** to non-text data
* Bridge the conceptual gap between **Recommender Systems and LLMs**
* Focus on **code understanding and data flow**, not just execution

---

## ğŸ“‚ Dataset Used

**Netflix Prize Dataset** (Kaggle)

* ~17,770 movies
* ~480,000 users
* ~100 million ratings (sampled for feasibility)

### Files Used:

* `movie_titles.csv` â€“ movie metadata
* `combined_data_1.txt` â€“ user ratings (sampled)

---

## ğŸ§  Core Concept

Traditional recommender systems predict ratings:

```
User + Movie â†’ Rating
```

This project predicts **next movie** instead:

```
(Movieâ‚, Movieâ‚‚, Movieâ‚ƒ) â†’ Movieâ‚„
```

This is the **same training objective used in GPT models**.

---

## ğŸ—ï¸ Project Architecture

```
Netflix Dataset
      â†“
Data Cleaning & Parsing
      â†“
Filter Positive Ratings (â‰¥ 4)
      â†“
User Watch Sequences
      â†“
GPT-style Inputâ€“Target Pairs
      â†“
Padding & Batching
      â†“
Transformer Encoder
      â†“
Next-Movie Prediction
```

---

## ğŸ§¹ Data Processing Steps

### 1. Load & Clean Data

* Parse raw Netflix text files
* Merge movie titles with ratings
* Store clean data in `netflix_final.csv`

### 2. Filter Ratings

Only ratings â‰¥ 4 are kept to model **user preferences**, not dislikes.

### 3. Tokenization

Movie IDs are converted to **continuous integer tokens**, just like word tokenization in NLP.

---

## ğŸ”„ Sequence Construction (Key Learning)

For a user who watched:

```
[10, 25, 80, 91]
```

Training samples created:

```
[10]        â†’ 25
[10, 25]    â†’ 80
[10, 25, 80]â†’ 91
```

This is **exactly how GPT is trained**.

---

## ğŸ“¦ Dataset Class

A custom PyTorch `Dataset`:

* Breaks user history into input â†’ target pairs
* Pads sequences to fixed length
* Outputs tensors suitable for Transformer input

---

## ğŸ¤– Model Architecture

### Transformer-Based Recommender

* **Embedding Layer**: Movie â†’ Vector
* **Transformer Encoder**:

  * Multi-head self-attention
  * Learns relationships between past movies
* **Linear Output Layer**:

  * Predicts probability over all movies
---

## ğŸ‹ï¸ Training Objective

* **Loss Function**: Cross-Entropy Loss
* **Optimizer**: Adam
* **Goal**: Maximize probability of correct next movie

---

## ğŸ” Recommendation Logic

Given a sequence of watched movies:

1. Convert titles â†’ movie tokens
2. Pad to fixed length
3. Pass through Transformer
4. Select top-K predicted movies
5. Convert tokens â†’ movie titles

---

## ğŸ“Š Example Output

**User watched:**

```
Reservoir Dogs
Dogma
Lilo and Stitch
```

**Recommended movies:**

```
North by Northwest
The Deer Hunter
Chasing Amy
```

Recommendations are based on **collaborative patterns across all users**.

---

## ğŸ§  Why This Project Matters

This project teaches:

* How Transformers work beyond text
* Why padding, batching, and tokenization exist
* How LLMs learn from sequences
* How attention captures long-term dependencies
---

## âš ï¸ Limitations

* Sampled dataset used for feasibility
* Not optimized for large-scale deployment

---

## ğŸš€ Future Improvements

* Add frontend / UI
---

## ğŸ§‘â€ğŸ« Learning Outcome

This project prioritizes:

* Understanding over complexity
* Flow of data over fancy code
* Conceptual clarity for LLM readiness

---



